{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 6: Boosting\n",
    "Course: MA06018, Machine Learning by professor Evgeny Burnaev <br\\>\n",
    "Author: Evgenii Egorov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_kit = np.asarray(Image.open('cat.png')).mean(axis=2)\n",
    "plt.figure(figsize=[10, 5]);\n",
    "plt.imshow(cat_kit, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def make_train_test(image, train_size_fraction):\n",
    "    h, w = image.shape\n",
    "    flat_image = image.reshape(-1)\n",
    "    \n",
    "    x_1 = np.arange(len(flat_image)) % w\n",
    "    x_2 = np.arange(len(flat_image)) // w    \n",
    "    data = np.array([x_1, x_2]).T \n",
    "    \n",
    "    target = flat_image\n",
    "    X_train, X_test, target_train, target_test = train_test_split(data, target, train_size=train_size_fraction,\\\n",
    "                                                                  test_size = 1 - train_size_fraction,\\\n",
    "                                                                  random_state=1011)\n",
    "    \n",
    "    return X_train, X_test, target_train, target_test, data\n",
    "\n",
    "\n",
    "def plot_prediction(original, prediction):\n",
    "    plt.figure(figsize=[20, 10]);\n",
    "    plt.subplot(121);\n",
    "    plt.imshow(original, cmap='gray');\n",
    "    plt.subplot(122);\n",
    "    plt.imshow((prediction).reshape(*original.shape), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, target_train, target_test, X = make_train_test(cat_kit, train_size_fraction=0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LR()\n",
    "regressor.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = regressor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(original=cat_kit, prediction=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Make same prediction with \n",
    "* DecisionTreeRegressor. Tune \"max_depth\" with crossvalidation at the set [3; 5; 10; 15]\n",
    "* RandomForestRegressor. Use n_estimators=100. Don't forget to set the n_jobs param, to use all your laptlop power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your MSE losses on https://goo.gl/forms/2wzmWffMXCnVnrFC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor as DTR\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = # your code\n",
    "tree = # your code\n",
    "regressor = # your code\n",
    "\n",
    "regressor.fit(X_train, target_train)\n",
    "prediction = regressor.predict(X)\n",
    "plot_prediction(original=cat_kit, prediction=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor as RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Only with 0.7% train size trees are such powerfull! Well, the reason is more at the image structure. So, it is why CNN is so popylar. Thanks to the nature. For details: <a href='https://arxiv.org/abs/1608.08225'>Why does deep and cheap learning work so well?</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garadient boosting in nuts and bolts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimization of loss function is optimization task. So, \"gradient boosting\" it is one of the possible ways to make greedy optimization.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& b_n(x) := \\text{a base algorithm from family of the algorithms $\\mathcal{A}$} \\\\\n",
    "& \\gamma_n(x) := \\text{scale or a weight of a base algorithm} \\\\\n",
    "& a_N(x) = \\sum\\limits_{n=0}^{N}\\gamma_n b_n(x) := \\text{target composition}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Algorithm for regression with some loss function $L(y, z)$ and the dataset $(x_i, y_i)_{i=1}^l$\n",
    "\n",
    "1. Init $b_0(x) = \\text{mean}(y_{train})$, $\\gamma_0 = 1$\n",
    "2. For n in N:\n",
    "    * Subporblem: $\\sum\\limits_{i=1}^{l}L\\left(y_i, a_{n-1}(x_i) + \\gamma_n b_n(x_i)\\right) \\to \\min\\limits_{b_{n}, \\gamma_n}$  \n",
    "    * Solution of the subproblem:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    & s_i = - \\frac{\\partial}{\\partial z} L|_{z = a_{n-1}(x_i)} \\\\\n",
    "    & b_n(x) = \\arg\\min\\limits_{b\\in\\mathcal{A}}\\sum\\limits_{i=1}^{l}(b(x_i) - s_i)^2 \\\\\n",
    "    & \\gamma_N = \\text{line-search or some-how selection}\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "return $a_N(x) = \\sum\\limits_{n=0}^{N}\\gamma_n b_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write MSE loss function and its gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_mse_grad(y, z):\n",
    "    # your code\n",
    "\n",
    "def loss_mse(y, z):\n",
    "   # your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Inplement boosting\n",
    "Insert your MSE losses on https://goo.gl/forms/nUH4YaKOPpZwG8sB3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import golden\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "def naive_boosting_predict(list_models, list_weights, point):\n",
    "    prediction = 0\n",
    "    for k in range(len(list_models)):\n",
    "        gamma = list_weights[k]\n",
    "        b = list_models[k]\n",
    "        if k > 0:\n",
    "            # your code\n",
    "        else:\n",
    "           # your code\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def naive_boosting_fit(N, X_train, y_train, regressor, loss_grad, loss, verbose=False, X_all=None, y_all=None):\n",
    "    b_0 = np.mean(y_train)\n",
    "    gamma_0 = 0.5\n",
    "    \n",
    "    list_models = []\n",
    "    list_weights = []\n",
    "    \n",
    "    list_models.append(b_0)\n",
    "    list_weights.append(gamma_0)\n",
    "    fig = plt.figure() # for plotting\n",
    "        \n",
    "    for n in range(N):\n",
    "        # your code\n",
    "        \n",
    "        gamma = golden(weight_func)\n",
    "        \n",
    "        \n",
    "        # for imaging during training\n",
    "        if verbose and n % 2 == 0:\n",
    "            all_predict = naive_boosting_predict(list_models, list_weights, X_all)\n",
    "            plt.ion()\n",
    "            plt.imshow((all_predict).reshape(*y_all.shape), cmap='gray');\n",
    "            plt.pause(0.05);\n",
    "            fig.canvas.draw()\n",
    "        #    \n",
    "            \n",
    "        list_models.append(deepcopy(regressor))\n",
    "        list_weights.append(gamma)\n",
    "    \n",
    "    return list_models, list_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_tree = # your code\n",
    "models, weights = # your code\n",
    "prediction = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(original=cat_kit, prediction=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_tree = # your code\n",
    "models, weights = # your code\n",
    "prediction = # your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(original=cat_kit, prediction=prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

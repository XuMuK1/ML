{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment -- 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your solutions within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import visualization and data processing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 (33 pt.): Convex Programs and Support Vector Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this **first two tasks** of this part of the homework you will implement\n",
    "your own Support Vectors Regression and then test on a simple real dataset.\n",
    "\n",
    "In the last task you are asked to solve a simple small NLP problem using\n",
    "support vector classfiers from [scikit-learn](http://scikit-learn.org/stable/modules/svm.html#classification).\n",
    "If you successfully complete the task, it will give you a ready-to-use and\n",
    "straighforward baseline for any NLP classification task of moderate size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Support Vector Regression\n",
    "\n",
    "$$\\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^l(\\xi_i + \\xi_i') \\to \\min_{w, b, \\xi_i, \\xi_i'}\\\\\n",
    "(w^Tx_i + b) - y_i \\leq \\epsilon - \\xi_i\\\\\n",
    "y_i - (w^Tx_i + b) \\leq \\epsilon - \\xi_i'\\\\\n",
    "\\xi_i \\geq 0\\\\\n",
    "\\xi_i' \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be written as empirical risk minimization problem with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Equivalent\n",
    "$$ \\frac{1}{2}\\|w\\|^2 + C\\sum_{i=1}^l h(\\hat{y}_i, y_i)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where\n",
    "$$ h(\\hat{y}_i, y_i) = \\max(0, |\\hat{y} - y| - \\epsilon ) $$\n",
    "is the $\\epsilon$-insensitive loss, and\n",
    "$$ \\hat{y}_i = (w, x_i) + b $$\n",
    "is the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our experiments we will use \"Cars\" dataset, which contains information about braking distances for several cars from the 1920s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_info = pd.read_csv('data/cars.csv', index_col=0, dtype=np.float)\n",
    "\n",
    "X = cars_info.speed.values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization leads to better convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "\n",
    "X_train = X_scaler.fit_transform(X)\n",
    "y_train = cars_info.dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(X, y_train, 'o')\n",
    "ax.set_xlabel('Speed')\n",
    "ax.set_ylabel('Distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (11 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please implement a function that calculates the current loss value and its gradient\n",
    "at the specified values of weights $w$, bias $b$, selected $C$ and $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eps_insensetive_loss(X, y, w, b, C=1.0, eps=0.1):\n",
    "    '''Calculate hinge loss function\n",
    "    :param: X -- numpy array of shape l x n\n",
    "    :param: y -- numpy array of shape l with values 1 and -1\n",
    "    :param: w -- numpy array of shape n\n",
    "    :param: b -- np.float64\n",
    "    :param: C -- np.float64 \n",
    "    '''\n",
    "    # Calculate Loss\n",
    "    loss = 0\n",
    "    ### BEGIN Solution\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    # Calculate Gradient for hinge loss    \n",
    "    w_grad = np.zeros_like(w)\n",
    "    b_grad = 0\n",
    "    ### BEGIN Solution\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    return loss, w_grad, b_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code finds optimal values for your loss function with simple constant step gradient descend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's metaparameters\n",
    "eps = 0.1\n",
    "C = 100\n",
    "\n",
    "# Gradient descent parameters\n",
    "step = 1e-4\n",
    "tolerance = 1e-3\n",
    "\n",
    "# Optimization logging\n",
    "converged = False\n",
    "index = 0\n",
    "training_loss_history = []\n",
    "training_loss_iteration = []\n",
    "\n",
    "# Parameters initialization\n",
    "w = np.zeros(1)\n",
    "b = 0\n",
    "loss, w_grad, b_grad = calculate_eps_insensetive_loss(X_train, y_train, w, b, C, eps)\n",
    "\n",
    "while not converged:\n",
    "    # Update parameters\n",
    "    new_w = w - step * w_grad\n",
    "    new_b = b - step * b_grad\n",
    "    # Calculate new loss and gradient\n",
    "    new_loss, w_grad, b_grad = calculate_eps_insensetive_loss(X_train, \n",
    "                                                    y_train, \n",
    "                                                    new_w, new_b, C, eps)\n",
    "    # Check convergence\n",
    "    if np.abs(loss - new_loss) < tolerance:\n",
    "        converged = True\n",
    "    \n",
    "    b, w, loss = new_b, new_w, new_loss\n",
    "    \n",
    "    # Update history\n",
    "    index += 1\n",
    "    if index % 10 == 0:\n",
    "        training_loss_history.append(new_loss)\n",
    "        training_loss_iteration.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = plt.gca()\n",
    "axis.plot(training_loss_iteration, training_loss_history)\n",
    "axis.set_xlabel('Iterations')\n",
    "axis.set_ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going calculate predictions for all speeds in range from $0$ to $30$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.arange(0., 30., 1).reshape(-1, 1)\n",
    "x_test = X_scaler.transform(x_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = x_test @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(x_grid, predictions, label='Predictions')\n",
    "ax.plot(X, y_train, 'o')\n",
    "ax.set_xlabel('Speed')\n",
    "ax.set_ylabel('Distance')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (11 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task the goal is to implement the SVR algorithm based on its dual problem version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### SVR Dual Problem\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        & \\underset{\\alpha', \\alpha}{\\text{minimize}}\n",
    "          & & \\frac{1}{2}(\\alpha' - \\alpha)^T  K (\\alpha' - \\alpha)\n",
    "              - (\\alpha' - \\alpha)^T Y\n",
    "              + \\epsilon (\\alpha' + \\alpha)^T \\mathbf{1}\n",
    "              \\,, \\\\\n",
    "        & \\text{subject to}\n",
    "          & & (\\alpha' - \\alpha)^T \\mathbf{1} = 0\n",
    "              \\,, \\\\\n",
    "        & & & \\alpha', \\alpha \\in [0, C]\n",
    "              \\,.\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Quadtratic Optimization Problem\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        & \\underset{x}{\\text{minimize}}\n",
    "          & & \\tfrac12 x^T P x + q^T x\n",
    "              \\,, \\\\\n",
    "        & \\text{subject to}\n",
    "          & & G x \\leq h\n",
    "              \\,, \\\\\n",
    "        & & & Ax = b\n",
    "              \\,.\n",
    "    \\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that instead of just $\\alpha$, like in the classification case, we have\n",
    "two vectors of coefficients: $\\alpha$ and $\\alpha'$. As an input space consider using\n",
    "stacked column vectors\n",
    "\\begin{equation}\n",
    "    \\begin{pmatrix}\n",
    "        \\alpha \\\\\n",
    "        \\alpha'\n",
    "    \\end{pmatrix}\\,.\n",
    "\\end{equation}\n",
    "\n",
    "**Hint**: You may find function `np.block` very usefull in this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we are going to use **cvxopt** library, which has solvers for different kind\n",
    "of convex optimization problems. Please notice that installation in Windows can\n",
    "be a little tricky and painful.\n",
    "\n",
    "http://cvxopt.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For more information about different types of optimization problem you can check\n",
    "this presentations. This material could be very helpful, however is not necessary\n",
    "for understatnding this course in Machine Learning.\n",
    "\n",
    "http://ee364a.stanford.edu/lectures/problems.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reformulate the Dual Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from cvxopt import matrix\n",
    "\n",
    "\n",
    "def formulate_problem(X, y, kernel, C, epsilon):\n",
    "    \"\"\"Formulate quadratic optimization\n",
    "    problem by defining matrices\n",
    "    :param: X np.array of size n_elem * n_dim with reaining values\n",
    "    :param: y np.array of size n_elem with labels 1 and -1\n",
    "    :kernel: kernel function wich accepts two matrices of objects\n",
    "    \"\"\"\n",
    "    quadratic_problem = {}\n",
    "\n",
    "    # REMEMBER: all values should be wrapped in `cvxopt.matrix`\n",
    "    # datatype (you can just use matrix(a) where a is `np.array`).\n",
    "    # Please be careful cvxopt accepts only FLOAT not INTEGER data\n",
    "\n",
    "    ### BEGIN Solution\n",
    "    quadratic_problem['P'] = None\n",
    "    quadratic_problem['q'] = None\n",
    "\n",
    "    quadratic_problem['G'] = None\n",
    "    quadratic_problem['h'] = None\n",
    "\n",
    "    quadratic_problem['A'] = None\n",
    "    quadratic_problem['b'] = None\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    return quadratic_problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Check the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module **metrics.pairwise** in **sklearn** has a very fast and well documented\n",
    "implementation of kernel matrix calculator. We fix the width of the RBF kernel at\n",
    "$1.0$ for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from functools import partial\n",
    "\n",
    "# the width of the RBF kernel is 1.0\n",
    "kernel = partial(rbf_kernel, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cvxopt** has powerful and versatile solvers for almost every convex\n",
    "optimization problem: linear, conic, and **quadratic**. We are using\n",
    "the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Optimization routine\n",
    "from cvxopt.solvers import qp\n",
    "\n",
    "qudratic_problem = formulate_problem(X_train, y_train.astype(float), kernel, C, eps)\n",
    "\n",
    "results = qp(**qudratic_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Extract optimal point form the results and force **small values** of the coefficients to **zero**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_stacked_alpha = np.array(results['x']).ravel()\n",
    "\n",
    "zero_threshold = 1e-6\n",
    "### BEGIN Solution\n",
    "# force negligible values to be exactly zero\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction\n",
    "\n",
    "Having a kernel function $K$ and optimal $(\\alpha, \\alpha')$, implement a function\n",
    "that computes the dot product $(w \\cdot x_i)$:\n",
    "\n",
    "$$(w \\cdot x_{test}) = \\sum_{i=1}^l(\\alpha_i' - \\alpha_i)K(x_i, x_{test}) \\,. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def kernel_dot_product(X_train, y_train, X_test, kernel, alpha):\n",
    "    '''Calculate scalar product with vector w\n",
    "    based on support vectors and dual coefficients\n",
    "    :param: X_train np.array with train data\n",
    "    :param: y_train np.array with train labels\n",
    "    :param: X_test np.array with test data\n",
    "    :parma: kernel the kernel function\n",
    "    :param: alpha stacked dual coefficients\n",
    "    '''\n",
    "    output = np.zeros(len(X_test))\n",
    "\n",
    "    ### BEGIN Solution\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constant Calculation\n",
    "\n",
    "The optimal constant $b$ can be derived from the theoretical properties of\n",
    "the support vectors:\n",
    "$$ b = y_i + \\epsilon - (w \\cdot x_i) \\, \\text{ if }\\, 0 < \\alpha_i < C \\,, $$\n",
    "and\n",
    "$$ b = y_i - \\epsilon - (w \\cdot x_i) \\, \\text{ if }\\, 0 < \\alpha_i' < C \\,. $$\n",
    "\n",
    "For example, on infinitely powerful hardware with infinite precision arithmetic\n",
    "you could have used any single support vector to get $b$. But it won't work IRL,\n",
    "where the most you can hope for is [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754)\n",
    "quadruple precision [floating point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic])\n",
    "(setting aside special libraries or hardware). In the **numpy-scipy-sklearn** stack\n",
    "on a typical x86-64 hardware your algorithms work in finite precision arithmetic, which\n",
    "is subject to roundoff errors. The arithmetic results are pretty accurate, but still\n",
    "**inexact**. Besides the numerical optimization algorithms (**cvxopt.qp**, **BFGS**,\n",
    "**Your own SGD** and the like) themselves aren't exact, and produce just an **extremely\n",
    "faithful** approximation of the optmial coefficients.\n",
    "\n",
    "Your task here is to use as many support vectors as possible to get an estimate of $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_bias(X_train, y_train, kernel, alpha, eps=0.1):\n",
    "    \"\"\"Calculates the bias \n",
    "    :param: X_train np.array with train data\n",
    "    :param: y_train np.array with train labels\n",
    "    :parma: kernel the kernel function\n",
    "    :param: alpha stacked dual coefficients'''\n",
    "    \"\"\"\n",
    "    b = 0.0\n",
    "\n",
    "    ### BEGIN Solution\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "b = calculate_bias(X_train, y_train, kernel, optimal_stacked_alpha, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the optimal $b$ and $\\alpha, \\alpha'$ to get the predictions and finally plot\n",
    "a nice picture of the SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kernel_dot_product(X_train, y_train, x_test, kernel, optimal_stacked_alpha) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "ax.plot(x_grid, predictions, label='Predictions')\n",
    "ax.plot(X, y_train, 'o')\n",
    "ax.set_xlabel('Speed')\n",
    "ax.set_ylabel('Distance')\n",
    "ax.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (11 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we are going to use SVM for text classification. We will work with\n",
    "dataset which  contains different tweets with the related conversation context.\n",
    "Some of the tweets are generated by bots, others -- written by humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/bot_or_not_train.csv', sep='\\t', index_col=0)\n",
    "\n",
    "test_data = pd.read_csv('data/bot_or_not_test.txt', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that context and response columns contains raw text, which is not\n",
    "directly suitable for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 (6 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract features we are going to use the **bag of words** techniques. Please read\n",
    "the documentation for `CountVectorizer` [in scikit](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
    "and do the following:\n",
    "\n",
    "* For both the context and the response generate a set of features using\n",
    "two separate count vectorizers.\n",
    "\n",
    "* Join this sets of features into one feature matrix (please notice that\n",
    "`CountVectorizer` returns Scipy [CSR sparse matrices](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)).\n",
    "\n",
    "* Pick the optimal $C$ for the **linear kernel** using cross valudation\n",
    "`GridSearchCV` and `ROC AUC` as the target metric.\n",
    "\n",
    "* Choose the optimal `gamma` and $C$ for the RBF kernel using cross valudation\n",
    "`GridSearchCV` targeting `ROC AUC` metric.\n",
    "\n",
    "* Take the **best models** with linear and RBF kernels, plot **their ROC curves**\n",
    "on the provided heldout test data on the same figure, and print their `ROC AUC`\n",
    "score. Which models work better on the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate feature matricies for the context and the response. Fill free to create additional\n",
    "feature extraction objects if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['human-generated']\n",
    "\n",
    "### BEGIN Solution\n",
    "x_context = None\n",
    "\n",
    "x_respose = None\n",
    "\n",
    "# Join the feature matrix\n",
    "x_train = None\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the response and contet test sets with their **relevant** feature encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_data['human-generated']\n",
    "\n",
    "### BEGIN Solution\n",
    "x_context_test = None\n",
    "\n",
    "x_response_test = None\n",
    "\n",
    "x_test = None\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best linear model among $C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# pick the best C\n",
    "\n",
    "best_linear_model = None\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best RBF kernel model among the hyperparameters\n",
    "$C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$ and $\\gamma \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# pick the best C and gamma\n",
    "\n",
    "best_kernel_model = None\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the `ROC` curves of both models and their `ROC AUC` scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the best model is...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "test_predictions = best_model.decision_function(x_test)\n",
    "test_score = roc_auc_score(y_test, test_predictions)\n",
    "print(\"Test ROC AUC = {}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 (5 pt.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are asked to **redo the last task (3.1)**, bit this time with more advanced\n",
    "features generated by [TfidfVectorizer](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "To do this you can use `TfidfVectorizer` from sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate feature matricies for context and response. Fill free to create\n",
    "additional feature extraction object if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "x_context = None\n",
    "\n",
    "x_response = None\n",
    "\n",
    "# Join the feature matrix\n",
    "x_train = None\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the response and contet test sets with their **relevant** feature encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "x_context_test = None\n",
    "\n",
    "x_response_test = None\n",
    "\n",
    "x_test = None\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best linear model among $C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# pick the best C\n",
    "\n",
    "best_linear_model = None\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best RBF kernel model among the hyperparameters\n",
    "$C \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$ and $\\gamma \\in [\\mathtt{1.e-3}, .., \\mathtt{1.e3}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "# pick the best C and gamma\n",
    "\n",
    "best_kernel_model = None\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the `ROC` curves of both models and their `ROC AUC` scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the best model is...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "test_predictions = best_model.decision_function(x_test)\n",
    "test_score = roc_auc_score(y_test, test_predictions)\n",
    "print(\"Test ROC AUC = {}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

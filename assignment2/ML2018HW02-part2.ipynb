{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment -- 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, write your implementation within the designated blocks:\n",
    "```python\n",
    "...\n",
    "### BEGIN Solution\n",
    "\n",
    "# >>> your solution here <<<\n",
    "\n",
    "### END Solution\n",
    "...\n",
    "```\n",
    "\n",
    "Write your theoretical derivations within such blocks:\n",
    "```markdown\n",
    "**BEGIN Solution**\n",
    "\n",
    "<!-- >>> your derivation here <<< -->\n",
    "\n",
    "**END Solution**\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 (57 + 2 pt.): Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you are asked to complete  several theoretical tasks as well as\n",
    "a couple of practical challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (8 pt.):  Some theory of Decision Trees\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& R_m := \\text{set of train objects at the node m} \\\\ \n",
    "& N_m := |R_m| \\\\\n",
    "& p_{mk} = \\frac{1}{N_m}\\sum\\limits_{(x_i, y_i)\\in R_m}[y_i = k] \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (2 pt.)\n",
    "\n",
    "Show that Gini Index can be written as\n",
    "\n",
    "$$\n",
    "\\sum_{k\\neq k'} p_{mk}p_{mk'}\n",
    "    = ???\n",
    "    = 1 - p_{m,k_m}, \\text{ where } k_m = \\arg\\max\\limits_{k}p_{mk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 (6 pt.)\n",
    "\n",
    "In fact one could view the $K$-mutliclass classification tree as a **piecewise\n",
    "constant vector function** $x\\mapsto T(x)$, where\n",
    "\\begin{equation}\n",
    "    T(x) = \\sum_{m=1}^J p_m [x\\in R_m] \\,,\n",
    "\\end{equation}\n",
    "where $[x\\in R_m]$ returns $1$ if $x\\in R_m$ and $0$ otherwise. Also $(R_m)_{m=1}^J$\n",
    "is the *tree-partition* of the feature space, induced by the **CART** (or any other)\n",
    "algorithm, and $p_m = (p_{mq})_{q=1}^K \\in [0, 1]^K$ is the vector of the empirical\n",
    "class probabilities, estimated on the training sample observations, that end up in\n",
    "the region $R_j$.\n",
    "\n",
    "For example, for any observed $x$ the vector $T(x)$ represents the estimated class\n",
    "probabilities in the respective tree-leaf. **Note** that the regions in the partition\n",
    "do **not overlap**, and $\\sum_{k=1}^K p_{mk} = 1$ for any $m$! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a fixed test example $x$ with the true class class label $t\\in \\{1.. K\\}$.\n",
    "Let $j=1..J$ be some leaf of a tree $T$.\n",
    "\n",
    "The **default** Decision Tree classifier, $\\hat{y}(\\cdot)$ **labels** all test\n",
    "observations $x$, that fall in $R_j$, by the **same class label**, determined\n",
    "by $\\hat{y}(x) = \\arg\\max_{q=1..K} p_{mq}$.\n",
    "\n",
    "Let's imagine an alternative rule that **assigns** a label to a test observation\n",
    "$x$, falling in tree-region $R_m$, **by choosing a label at random** according\n",
    "to the *estimated* distribution $p_m$:\n",
    "$$ y(x)\n",
    "    = \\begin{cases}\n",
    "        1\\,, & \\text{ with prob. } p_{m1} \\,, \\\\\n",
    "        \\ldots & \\\\\n",
    "        K\\,, & \\text{ with prob. } p_{mK} \\,. \\\\\n",
    "      \\end{cases}\n",
    "$$\n",
    "\n",
    "Show that the **new labelling rule** does not improve the theoretical misclassification\n",
    "error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "<!-- // BEGIN Solution -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- END Solution -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (11+2 pt.): Practice with Bootstrap and Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will be asked to make a couple of plots depicting the variance\n",
    "of the predicted probability of class $1$ by some classification algorithm.\n",
    "\n",
    "But first generate the toy [Crescent Moons dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=300, shuffle=True, noise=0.05, random_state=1011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (8+2 pt.)\n",
    "\n",
    "You need to somehow estimate a function $x\\mapsto \\mathbb{V} T(x)$ for a class of\n",
    "estimators $T$, defined by\n",
    "$$ \\mathbb{V} T(x) = \\mathbb{V}_{S \\sim D^m} T(x; S)\n",
    "    \\,, $$\n",
    "where $T(x; S)$ means \"classifier of class $T$ returned by a learning algorithm applied\n",
    "to the training sample $S$ and evaluated at a point $x$\". The notaion $S \\sim D^m$ means\n",
    "\"a randomly drawn iid sample of size $m$ from the data distribution $D$\". The key problem\n",
    "is that you **never know** $D$ but you have a ready training sample $(X, y)$ from it...\n",
    "\n",
    "Please, implement a **procedure** which uses **bootstrap** to construct an estimate of\n",
    "$\\mathbb{V} T(\\cdot)$ on a fixed common test dataset for the provided classifier. Please\n",
    "refer to **Seminar 5** for pictures, **explanation** and examples, and to **scikit's\n",
    "API** documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus** You will get an extra $+2$ pt. if instead of re-implementing everything\n",
    "yourself, you figure out how to use sklearn's `BaggingClassifier`. You are in for a\n",
    "certain feeling of satisfaction with how **clever** sklearn's API is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "def bootstrap_predictions(estimator, X, y, X_test, n_bootstrap=101):\n",
    "    \"\"\"Bootstrap a given classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object\n",
    "        A classifier instance with sklearn-compatible interface.\n",
    "\n",
    "    X : array, shape = (n_samples, n_features)\n",
    "        The X part of the full dataset.\n",
    "\n",
    "    y : array, shape = (n_samples,)\n",
    "        The target labels of the full dataset.\n",
    "\n",
    "    X_test : array, shape(n_test_samples, n_features)\n",
    "        The test data to measure theoutput of bootstrapped estimators on.\n",
    "\n",
    "    n_bootstrap : int, nonnegative\n",
    "        The number of bootstrap replication of `estimator` to make.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    proba : array, shape=(n_test_samples, n_bootstrap), dtype=float\n",
    "        The matrix of bootstrapped outputs of the classifier.\n",
    "\n",
    "    bag : list\n",
    "        The list of bootstrap replications of the classifier.\n",
    "\n",
    "    Details\n",
    "    -------\n",
    "    The `(X, y)` full dataset is used to generate bootstrap samples. Each one\n",
    "    of `n_bootstrap` samples is used to train a separate copy of the provided\n",
    "    `estimator`. Each classifier form resulting set of bootstrapped estimators\n",
    "    is applied to `X_test` and the output is recorded in `proba` array.\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN Solution\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    return output, bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 (3 pt.)\n",
    "\n",
    "Define a regular $2$-D mesh $51 \\times 51$ that **covers the box**, within which\n",
    "the original dataset resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(a, b, factor=0.2):\n",
    "    \"\"\"Make a wider interval defined by the endpoints.\"\"\"\n",
    "    return a - abs(a) * factor, b + abs(b) * factor\n",
    "\n",
    "X_l, X_h = X.min(axis=0), X.max(axis=0)\n",
    "xx0, xx1 = np.meshgrid(np.linspace(*expand(X_l[0], X_h[0]), num=51),\n",
    "                       np.linspace(*expand(X_l[1], X_h[1]), num=51))\n",
    "\n",
    "X_grid = np.c_[xx0.ravel(), xx1.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the bootstrap estimate ($101$ replications) of the variance of the predicted\n",
    "probability of class $1$ at each point of the $2$-d grid. As the base classifiers\n",
    "use the **Decision Tree Classifier** with different settings for `min_samples_leaf`\n",
    "* $3$, $10$, $75$.\n",
    "\n",
    "You must create three separate contour plots. Make sure to properly label your\n",
    "axes and provide meaningful titles for each [subplot](https://matplotlib.org/api/axes_api.html#axis-labels-title-and-legend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=1, figsize=(13, 4))\n",
    "for ax, min_samples_leaf in zip(axes, [3, 10, 75]):\n",
    "    ### BEGIN Solution\n",
    "\n",
    "\n",
    "    ### END Solution\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=4)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (20 pt.):  Boosting and its theory\n",
    "\n",
    "Minimization of a loss function is an optimization task, and \"Gradient Boosting\"\n",
    "is one of the many methods to perform optimization. It shoould be noted that it\n",
    "uses **greedy** approach and thus, like greedy algorithms in CS, may produce\n",
    "results that are not *globally* optimal.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    & b_n(x) := \\text{the best base model from the family of the algorithms $\\mathcal{A}$} \\\\\n",
    "    & \\gamma_n(x) := \\text{scale or weight of the new model} \\\\\n",
    "    & a_N(x) = \\sum_{n=0}^N \\gamma_n b_n(x) := \\text{the final composite model}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Algorithm\n",
    "\n",
    "Consider a loss loss function $L(y, z)$ for target $y$ and prediction $z$, and let\n",
    "$(x_i, y_i)_{i=1}^l$ be out train dataset for a regression task. \n",
    "\n",
    "\n",
    "1. Initialize $a_0(x) = \\hat{z}$ with the **flat constant prediction**\n",
    "$\\hat{z} = \\arg\\min\\limits_{z \\in \\mathbb{R}} \\sum_{i=1}^l L(y_i, z)$;\n",
    "2. For $n$ from `1` to `n_boost_steps` do:\n",
    "    * Solve the current subporblem $G_n(b_n, \\gamma_n) \\to \\min\\limits_{b_{n}, \\gamma_n}$\n",
    "    where \n",
    "    $$ G_n(b, \\gamma) = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr) $$\n",
    "    with the following method:\n",
    "    \\begin{align}\n",
    "      & s_i = - \\frac{\\partial}{\\partial z} L(y_i, z) \\Big\\vert_{z=a_{n-1}(x_i)}\n",
    "          \\\\\n",
    "      & b_n(x) = \\arg\\min\\limits_{b\\in\\mathcal{A}}\\sum_{i=1}^l \\bigl(b(x_i) - s_i\\bigr)^2\n",
    "          \\\\\n",
    "      & \\gamma_n = \\arg\\min_\\gamma G_n(b_n, \\gamma)\n",
    "          \\\\\n",
    "      & a_n(x) = a_{n-1}(x) + \\gamma_n b_n(x)\n",
    "    \\end{align}\n",
    "3. return $a_N(x) = a_0(x) + \\sum_{n=1}^N \\gamma_n b_n(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1  (2 pt.)\n",
    "\n",
    "Consider the *logistic loss* for classification task $L(y,z) = \\log\\bigl(1 + e^{-y z}\\bigr)$,\n",
    "where $y \\in \\{-1, +1\\}$. Derive the **gradient** for it with respect to $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 (3 pt.)\n",
    "\n",
    "At the $n$-th step of Garient Boosting ($n \\geq 1$ we compute the \"residuals\"\n",
    "$(s_i)_{i=1}^l$ and learn $x\\mapsto b_n(x)$ with a regression algorithm $\\mathcal{A}$\n",
    "applied to the dataset $(x_i, s_i)_{i=1}^l$. For the next two tasks **assume\n",
    "that we have already perfomed these substeps**.\n",
    "\n",
    "Derive the **optimal value** of $\\gamma_n$ for *MSE* loss function $L(y, z) = \\tfrac12 (y - z)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3 (15 pt.)\n",
    "\n",
    "Let $S = (x_i, y_i)_{i=1}^l$ be a sample for a classification task $y_i \\in \\{-1, +1\\}$.\n",
    "\n",
    "The **AdaBoost** algorithm can be regarded as a gradient boosting algorithm\n",
    "with the exponential loss $L(y,z) = e^{-y z}$. Consdier the state of **AdaBoost**\n",
    "at the $(T-1)$-step\n",
    "$$ G_{T-1}(b_T, \\gamma_T)\n",
    "    = \\sum_{i=1}^l L\\bigl(y_i, a_{n-1}(x_i) + \\gamma b(x_i)\\bigr)\n",
    "    = \\sum_{i=1}^l \\underbrace{\\exp(- y_i a_{T-1}(x_i))}_{w^{T-1}_i}\n",
    "        \\exp(- y_i \\gamma_T b_T(x_i))\n",
    "    \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3.1 (4 pt.)\n",
    "\n",
    "Derive the next weights $w^T_i$ used in $G_T$ at the stage $T$ as a function\n",
    "of the learnt classifier $b_T$ and the current weights $w^{T-1}_i$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3.2 (9 pt.)\n",
    "\n",
    "Compute the ratio of weights $(w^T_i)_{i=1}^l$ between the normal and outlier\n",
    "samples in $S$. Propose a **formal definition of being an outlier**, and reflect\n",
    "on the value of *margin* for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3.2 (2 pt.)\n",
    "\n",
    "Conclude about **sensitivity** of Adaboost to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGIN Solution\n",
    "\n",
    "\n",
    "\n",
    "**END Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (18 pt.): Practical Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to implement a boosting algorithm, compare speed of\n",
    "different popular boosting libraries, and use boosting to measure feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1 (9 pt.): Boosting Classification on a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a toy dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=300, shuffle=True, noise=0.05, random_state=1011)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is:\n",
    "1. **(5 pt.)** Implement gradient boosting algorithms with **logistic loss**\n",
    "and labels $y\\in \\{-1, +1\\}$;\n",
    "2. **(2 pt.)** **Plot the decision boundary** on a $2$-d grid;\n",
    "3. **(2 pt.)** Estimate the accuracy **score** on the test dataset, as well\n",
    "as other classification metrics, that you can think of;\n",
    "    \n",
    "For basic implementation please refer to the 6th seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN Solution\n",
    "\n",
    "### END Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2 (9 pt.): Measuring the Speed and Performance\n",
    "\n",
    "Please make sure to install the following powerful packages for boosting:\n",
    "* [xgboost](https://anaconda.org/conda-forge/xgboost)\n",
    "* [lightgbm](https://anaconda.org/conda-forge/lightgbm)\n",
    "* [catboost](https://tech.yandex.com/catboost/doc/dg/concepts/python-installation-docpage/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you are asked to compare the **training time** of the **GBDT**, the\n",
    "Gradient Boosted Decision Trees, as implemeted by different popular ML libraries.\n",
    "The dataset you shall use is the [UCI Breast Cancer dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29).\n",
    "You should study the parameters of each library and establish the **correspondence**\n",
    "between them.\n",
    "\n",
    "The plan is as follows:\n",
    "1. **(3 pt.)** take the **default** parameter settings, measure the training time, and plot\n",
    "the ROC curves;\n",
    "2. **(6 pt.)** use grid search with the $3$-fold cross valiadation to choose the best model.\n",
    "Then measure the training time as a function of (separately) **tree depth** and **the\n",
    "number of estimators in the ensemble**, finally **plot the ROC** curves of the best\n",
    "models.\n",
    "\n",
    "You need to make sure that you are comparing **comparable** classifiers, i.e. with\n",
    "**the same tree and ensemble hyperparameters**.\n",
    "\n",
    "Please plot **three** ROC curves, one per library, on the same **one plot**\n",
    "with a *comprehensible [legend](https://matplotlib.org/users/legend_guide.html)*.\n",
    "\n",
    "A useful command for timing is IPython's [**timeit** cell magic](http://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4,\n",
    "                                                    random_state=0x0BADBEEF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/> <!--Intentionally left blank-->"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
